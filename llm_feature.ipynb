{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLm Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask\n",
    "To test weather mask's self attention has great impact over output logit\n",
    "\n",
    "the mask value of itself won't have impact on itself.\n",
    "\n",
    "That means the mask of i_th 1 in [1,1,...,1] don't have impact on P(x_i | x_i-1,...,x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from utils.utils import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "inference_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "inference_model.to(device)\n",
    "string = \"The Project Gutenberg eBook of Complete Prose Works This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "        inputs = tokenizer(tokenizer.bos_token + string, return_tensors=\"pt\")\n",
    "        inputs.to(device)\n",
    "        '''\n",
    "        inputs[\"input_ids\"]: tensor; shape of inputs[\"input_ids\"] is [batchsize, token#]\n",
    "        token_list: tensor; shape of token_list is [token#]\n",
    "        '''\n",
    "        token_list = inputs[\"input_ids\"].squeeze(0)\n",
    "        '''\n",
    "        maxmium of i in this loop is *len(token_list) - 2*\n",
    "        # in this token_list is len(token_list)\n",
    "        maxmiun of index is len(token_list) - 1\n",
    "        '''\n",
    "        for i in range(1, len(token_list) - 1):\n",
    "            current_token_index = token_list[i + 1]\n",
    "            current_inputs = deepcopy(inputs)\n",
    "            '''\n",
    "            Tensor sliding:\n",
    "                [:] all elements\n",
    "                [n:] last n elements\n",
    "                [:n] first n elements\n",
    "            Shape of current_inputs[\"attention_mask\"] and current_inputs[\"input_ids\"]:\n",
    "                [batchsize, i + 1]\n",
    "            '''\n",
    "            current_inputs[\"input_ids\"] = current_inputs[\"input_ids\"][:, :(i + 1)]\n",
    "            current_inputs[\"attention_mask\"] = current_inputs[\"attention_mask\"][:, :(i + 1)]\n",
    "            \n",
    "            outputs = inference_model(**current_inputs, labels=current_inputs[\"input_ids\"])\n",
    "            '''\n",
    "            Shape of logits:\n",
    "                +[batchsize, tokens#, 50257(dict alpha/beta#)]\n",
    "            '''\n",
    "            logits = outputs.logits\n",
    "            current_inputs[\"attention_mask\"][:, -1] = 0\n",
    "            outputs_modified = inference_model(**current_inputs, labels=current_inputs[\"input_ids\"])\n",
    "            logits_modified = outputs_modified.logits\n",
    "\n",
    "            print(torch.equal(logits[0,-2], logits_modified[0,-2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compress files main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--maxdps MAXDPS] [--device DEVICE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"ee3bcf82-1cc9-42ba-96b8-85f5e03383ac\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=/ubc/ece/home/rl/grads/qihangz/.local/share/jupyter/runtime/kernel-v2-415951vtD5Evj7MTfN.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import *\n",
    "from utils.enwiki9_dataset import *\n",
    "from mpmath import*\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--maxdps\", default=1600, type=int)\n",
    "parser.add_argument(\"--device\", type=str, default=\"gpu\")\n",
    "args = parser.parse_args()\n",
    "torch.set_printoptions(precision=50)\n",
    "mp.dps = args.maxdps\n",
    "\n",
    "if args.device == \"gpu\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "elif args.device == \"cpu\":\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = model.to(device) \n",
    "\n",
    "init_state = True\n",
    "global_bound = ac_bound(torch.tensor(0.0), torch.tensor(1.0))\n",
    "\n",
    "acc_count = 0\n",
    "all_count = 0\n",
    "global_compress_info = compress_info(0, 0)\n",
    "with open('dataset/8813.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        stress_print(line)\n",
    "        is_acc, current_compress_info = encoder_and_decoder(str_to_encoder = line, model=model, tokenizer=tokenizer, device = device)\n",
    "        all_count += 1\n",
    "        if is_acc:\n",
    "            acc_count += 1 \n",
    "            global_compress_info.update_compress_info(current_compress_info)\n",
    "        pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slicing operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "list_example = [\n",
    "    [11,12],\n",
    "    [21,22]\n",
    "]\n",
    "tensor_example = torch.tensor(list_example)\n",
    "print(1,tensor_example)\n",
    "print(2,tensor_example[:])\n",
    "print(3,tensor_example[:,])\n",
    "print(4,tensor_example[:, 1:])\n",
    "print(5,tensor_example[:, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range(N):\n",
    "maxmium of i is N -1\n",
    "'''\n",
    "for i in range(10):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
